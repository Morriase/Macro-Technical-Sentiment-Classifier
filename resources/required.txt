Deep Research on the Macro-Technical Sentiment Classifier: An Open-Source XGBoost-LSTM Hybrid Architecture
I. Strategic Overview: Rationale and Foundational Architecture
The development of advanced financial classification systems requires the integration of diverse and complex data streams—macroeconomic indicators, traditional technical analysis metrics, and derived linguistic sentiment—to predict market direction. Financial time series data is characterized by extreme volatility, nonlinearity, high noise, and strong temporal dependencies. Conventional models often fail to generalize across varying financial contexts and struggle to identify nonlinear dynamics, leading to suboptimal performance. The proposed solution utilizes a hybrid machine learning approach, specifically combining XGBoost and Long Short-Term Memory (LSTM) networks, to strategically exploit their complementary strengths and overcome the limitations inherent in standalone models. This integration is engineered to enhance predictive performance, robustness, and generalization capabilities in classification tasks predicting directional movements (e.g., Up/Down).   

A. The Hybrid Advantage: XGBoost and LSTM Synergy
The efficacy of the hybrid model stems from allocating specific predictive tasks to the algorithm best suited for that data structure.

1. XGBoost’s Role (Structured Data & Feature Importance)
Extreme Gradient Boosting (XGBoost) is a Gradient Boosting Decision Tree (GBDT) framework recognized for its efficiency, scalability, and ability to model complex, non-linear feature interactions within tabular datasets. In this architecture, XGBoost is dedicated to handling the structured, often static, features derived from macroeconomic reports and technical indicators. Its operational roles are multifaceted:   

First, XGBoost excels at identifying and modeling the non-linear relationships that define feature interaction within the structured data domain. These interactions are crucial for predicting short-term market reactions based on current market metrics.   

Second, XGBoost provides robust tools for feature importance analysis, notably through SHAP (SHapley Additive exPlanations) values. This capability is critical during the development phase, allowing for iterative feature optimization by identifying and eliminating irrelevant or noisy inputs, thereby reducing the dimensionality of the feature space and improving model efficiency.   

Third, and most importantly in the stacking architecture, XGBoost serves as the final meta-classifier, integrating the outputs of the LSTM sequence model with the raw tabular features.   

2. LSTM’s Role (Temporal Dynamics and Sequence Modeling)
Long Short-Term Memory (LSTM) networks are a specialized type of Recurrent Neural Network (RNN) designed specifically to manage sequential data and capture long-term dependencies, effectively mitigating the vanishing gradient problem. Financial time series data inherently possesses temporal autocorrelation and sequence patterns, which LSTMs are uniquely positioned to model.   

The LSTM component focuses on features where temporal order is paramount, particularly sequences derived from sentiment analysis and lagged market data. LSTMs address the time-dependent nature of financial data , capturing persistence and long-term patterns, which studies suggest may correspond to trends spanning periods such as 22 trading days. The ability of the LSTM to encode sequential patterns provides a specialized prediction stream that complements the instantaneous, non-linear insights derived by XGBoost.   

B. Core Architectural Strategy: Stacking Ensemble
The fundamental approach for combining these distinct models is the Stacking Ensemble (Stacked Generalization). This technique leverages the strengths of multiple diverse base models to make a final, more accurate prediction by using a Level-1 meta-model.   

While alternative combination methods exist, such as cascading (where XGBoost performs initial feature selection, and LSTM makes the prediction) , the stacking methodology offers greater operational robustness. In a stacking ensemble, the predictions (or hidden states) from the base models (Level 0) become the input features for the final meta-classifier (Level 1). By utilizing XGBoost as the Level-1 meta-model, the system benefits from XGBoost's proven non-linear classification capabilities to learn how and when to best combine and weigh the outputs from the LSTM (sequence trends) and potentially another XGBoost base model (tabular interactions). This structured integration effectively balances the specialized predictive capabilities of deep sequence learning with the robust classification power of gradient boosting.   

II. Open-Source Data Acquisition and Infrastructure Setup
A foundational constraint for this project is the strict adherence to fully open-source tools and utilities, avoiding proprietary solutions. This mandates careful selection of both the software stack and data sources.

A. Open-Source Infrastructure Stack
The implementation relies on the established Python data science and machine learning ecosystem.

Deep Learning Framework: PyTorch is selected for the LSTM implementation. While TensorFlow offers extensive libraries and is often preferred for production scalability, PyTorch is advantageous in the research and development phase of a novel hybrid model. PyTorch offers a dynamic computation graph and a more Pythonic syntax, which significantly simplifies debugging and allows for more flexible, rapid prototyping and customization of the LSTM network structure. For deployment needs, the PyTorch ecosystem has matured significantly, offering tools like TorchServe to close the gap on production deployment capabilities previously dominated by TensorFlow.   

Gradient Boosting Framework: The xgboost Python package provides the native and scikit-learn compatible interfaces required for implementing both the base XGBoost classifier and the Level-1 meta-classifier.   

Feature Engineering and Utility Libraries: Core data manipulation will be handled by pandas and numpy. The essential ML toolkit provided by scikit-learn (for pipelines, scaling, and evaluation metrics) is necessary.   

Technical Indicator Library: TA-Lib is mandated as the open-source library for generating technical analysis indicators. It provides a robust, standardized API for calculating over 150 indicators, including MACD, RSI, and Bollinger Bands, which are foundational inputs for technical features.   

Natural Language Processing (NLP): The Hugging Face Transformers library provides access to state-of-the-art open-source pre-trained language models, specifically FinBERT.   

Scalability Consideration: If the data volume (especially high-frequency technical data or extensive historical sentiment corpora) exceeds the capacity of a single machine, Apache Spark should be integrated. Spark is a unified analytics engine designed for large-scale data processing, offering built-in modules for streaming and machine learning, which is a critical capability when processing years of transaction data or running large-scale risk analyses in finance.   

B. Data Sourcing Strategy (Macro, Technical, Sentiment)
The model relies on three distinct pillars of data, each sourced from available public and free platforms:

1. Technical Data
Technical indicators require reliable time series of price and volume data. Open-source or free-tier APIs are utilized:

Sources: Alpha Vantage and Finnhub offer free access to real-time and historical stock market data, including OHLCV (Open, High, Low, Close, Volume) quotes and technical indicators.   

Core Series: The analysis must focus on key market proxies, such as the SPDR S&P 500 ETF Trust (SPY), as a representation of overall market performance. The raw OHLCV series forms the basis for technical indicator calculation.   

2. Macroeconomic Data
Macroeconomic features provide crucial exogenous context that traditional technical models often neglect.   

Sources: DBnomics serves as a free aggregator for publicly available economic data from national and international statistical institutions. Additionally, governmental portals such as Data.gov (US) and data.europa.eu (EU) offer vast catalogs of economic and financial datasets, including GDP estimates and consumer price indexes.   

Key Indicators: The strategy must focus on key economic drivers, including the U.S. Dollar Index (DXY), United States Treasury PAR Yield Curve Rates (critical for measuring economic expectations), and official central bank documents (FED/ECB) which directly correlate with local interest rates and monetary policy.   

Data Latency Management: A significant challenge with macro indicators (e.g., quarterly GDP, monthly CPI, or central bank minutes) is their low publication frequency and inherent latency. To ensure predictions are based only on information known at the time of forecasting (avoiding look-ahead bias), these low-frequency values must be properly time-stamped and carried forward across daily observations until the next release date.   

3. Sentiment Data (Linguistic Corpus)
Sentiment data derived from financial text is vital as it reflects investor mood and can impact stock prices before traditional indicators fully respond.   

Corpus Source: The Financial News Sentiment Prediction ID (FNSPID) dataset provides an extensive, open-source corpus of financial news articles and headlines with associated sentiment labels. This dataset is essential for validating and potentially fine-tuning the specialized financial NLP model.   

NLP Tooling: The Hugging Face Transformers library provides access to specialized models like FinBERT. FinBERT is a variant of the BERT model pre-trained specifically on a large financial corpus and fine-tuned on benchmark datasets like the Financial PhraseBank, ensuring domain-specific understanding of financial jargon and contextual ambiguity.   

III. Comprehensive Feature Engineering Blueprint
The predictive performance of any machine learning model, particularly in finance, is highly dependent on the quality and structure of its input features. Transforming raw financial data into meaningful inputs is paramount; as is commonly stated, "garbage in, garbage out".   

A. Technical Indicator Feature Generation
Technical features derived from OHLCV data capture endogenous market momentum, trend strength, and volatility.

Core Feature Set: The TA-Lib wrapper provides the necessary functions to calculate standard, high-impact indicators. These include momentum indicators like the Relative Strength Index (RSI) and Moving Average Convergence Divergence (MACD), trend indicators such as various Moving Averages, and volatility measures like the Average True Range (ATR) and Bollinger Bands.   

Lagged Price Features (Autoregression): While LSTMs inherently handle sequence, simpler autoregressive components improve model performance, particularly for the tree-based component. Lagged values of the target variable (e.g., C 
t−1
​
 ,C 
t−2
​
 ) and key prices (Open, High, Low, Close, Volume) should be included as static features for the XGBoost component.   

Return Measures: Calculate key metrics such as realized volatility over fixed windows and annualized returns for daily, weekly, and monthly periods. These volatility measures provide important context regarding market risk, complementing simple price prediction features.   

B. Macroeconomic Feature Structuring and Alignment
Macro features introduce critical external information. Proper handling of these features, which often differ significantly in frequency from daily technical data, is vital.   

Processing Heterogeneous Data: Data collected from global institutions  must be homogenized to the desired daily prediction frequency. For instance, quarterly GDP figures or monthly CPI releases must be explicitly held constant across all daily observations until the subsequent release date to maintain causal integrity.   

Key Macro Indicators: Features should include DXY, various points on the US Treasury PAR Yield Curve, and potentially derived features from central bank communications, which have demonstrated correlation with local interest rate movements.   

Advanced Feature Reduction (PCA): Macroeconomic indicators are frequently multicollinear. Applying Principal Component Analysis (PCA) to this feature subset prior to feeding it into the classifiers serves two purposes: it extracts meaningful, orthogonal components, and it reduces dimensionality, mitigating noise. This process ensures the XGBoost component receives optimized, robust macroeconomic input vectors.   

C. Advanced Sentiment Feature Extraction (NLP)
Linguistic features derived from financial text must move beyond simple dictionary lookups, which struggle with technical jargon and contextual ambiguity.   

Domain-Specific NLP via FinBERT: The use of FinBERT  is crucial, as it provides a model pre-trained to understand the specific language and context of finance, yielding superior performance in financial sentiment classification compared to generic models.   

Feature Vector Generation for Classification:

Softmax Sentiment Scores: Daily news aggregates are fed into FinBERT, and the resulting softmax output (probability vectors for Positive, Negative, and Neutral classifications) are extracted. These three probabilities form a foundational sentiment feature set.

Embedding Features: Beyond simple scores, the final hidden state or pooled output of the FinBERT Transformer encoder can be used to generate dense numerical embeddings. These high-dimensional vectors capture the nuanced semantic content of the news aggregated for that day. Dimensionality reduction (again, via PCA) should be applied to these embeddings to produce a refined, high-information feature set suitable for input into the LSTM.   

Time-Weighting and Lag Modeling: Recognizing that news impact often has both immediate and delayed effects on prices , the sentiment scores must be temporally aggregated. Exponential Moving Averages (EMAs) of the daily sentiment probabilities are generated to represent the decaying influence of past news over time, ensuring the model accounts for the persistence of investor sentiment. This sequence of time-decayed sentiment scores is uniquely suited for the LSTM component.   

D. Data Preparation for Hybrid Input
Successfully integrating tree-based and deep learning components requires meticulous data alignment and scaling. While XGBoost is largely invariant to feature scaling, LSTMs and other neural networks are highly sensitive to the magnitude of inputs.   

Normalization and Scaling: All features designated for the LSTM network (including Macro, Technical, and Sentiment features) must be scaled to a consistent range, typically using MinMaxScaler (to $$) or StandardScaler. This prevents features with larger numerical magnitudes from unduly dominating the network’s learning process.   

Sequence Transformation (LSTM Input): A critical step is the transformation of the 2D tabular feature matrix, which is used by XGBoost, into the 3D tensor format required by the LSTM network. The required format is: $ [N_{\text{samples}}, L, N_{\text{features}}] $ Where N 
samples
​
  is the number of predictions, L is the defined sequence length (look-back window), and N 
features
​
  is the number of features being input to the LSTM at each timestep.   

Sequence Length (L): The choice of the sequence length L (the number of past timesteps provided to the LSTM for each prediction) is a key hyperparameter that must be optimized via time series cross-validation. Common values range from 10 to 40 days, representing short-to-medium term memory.   

IV. The Hybrid XGBoost/LSTM Architecture Implementation
The final model utilizes the Stacking Ensemble paradigm, integrating outputs from two specialized Level-0 base models before the final decision is made by the Level-1 meta-classifier.

A. Base Model 1: The LSTM Sequence Encoder
The LSTM network is implemented using the PyTorch framework. Its purpose is to encode the temporal dependencies present in the time-series data.

Architecture: A robust deep learning approach for financial time series often involves a two-layer LSTM structure.   

The first LSTM layer typically returns sequences (return\_sequences=True), passing a processed temporal state vector to the next layer.

The second LSTM layer returns only the final hidden state (return\_sequences=False), which summarizes the entire input sequence (L timesteps) into a fixed-length vector.

Regularization techniques, such as Dropout layers after each LSTM layer and Recurrent Dropout within the LSTM cells, are necessary to prevent the model from overfitting to the noisy financial data.

Feature Input: The LSTM receives the 3D sequence-formatted, normalized technical, macro, and sentiment features.

LSTM Output for Stacking: The crucial output passed to the meta-model is the final hidden state vector derived from the sequence processing, ideally just before the final sigmoid/softmax classification layer. This vector is a highly distilled, sequence-aware representation of the market state. Alternatively, the raw prediction probability (logit or softmax output) can be used.   

B. Base Model 2: The XGBoost Feature Learner
While the Level-1 model is XGBoost, an XGBoost Base Model (Level 0) provides a robust classification prediction derived solely from the instantaneous, non-sequential characteristics of the market.   

Architecture and Input: The XGBoost Base Classifier is implemented using the Scikit-learn API (XGBClassifier). It takes the 2D tabular data, including the raw Macro features, current Technical indicators, and the time-weighted aggregate Sentiment scores. This model utilizes the Direct Approach, predicting the target at T+1 using features available at time T.   

Hyperparameter Tuning: Key XGBoost parameters such as max_depth, learning_rate, n_estimators, and regularization terms (lambda, alpha) must be rigorously tuned, ideally within the walk-forward validation scheme, to optimize performance and prevent overfitting.   

C. Level-1 Meta-Classifier: XGBoost Stacking Implementation
The hybrid model culminates in the XGBoost Meta-Classifier, which is responsible for harmonizing the predictions and features from the base models.   

Training Data for Meta-Model (X 
meta
​
 ): The input features for the Level-1 model consist of several elements:

The final encoded output vector or prediction probabilities from the LSTM Base Model.

The prediction probabilities from the XGBoost Base Model.

A selected subset of high-importance raw features, typically identified during the XGBoost Base Model’s feature importance analysis, ensuring the meta-model retains access to critical domain information.   

The Role of Out-of-Fold Predictions: To ensure the meta-model generalizes correctly and avoids data leakage, it must not be trained on the same data used to train the base models. Consequently, the input features for the meta-model (X 
meta
​
 ) are composed exclusively of out-of-fold predictions generated during the Walk-Forward Validation phase (Section V).   

Final Output: The Meta-XGBoost, having learned the optimal method of combining the specialized base model predictions, produces the final binary classification decision (e.g., predicted directional movement Up or Down). This stacking strategy is validated by its ability to compensate for the shortcomings of individual models, resulting in superior prediction accuracy.   

V. Robust Time-Series Validation and Performance Evaluation
In financial modeling, non-stationarity, volatility clustering, and serial dependence mean that standard K-fold cross-validation (which relies on shuffling data) is fundamentally flawed. Shuffling destroys temporal dependencies, leading to training on future data and testing on past data, which renders the performance estimate meaningless for real-world deployment. Therefore, rigorous, time-aware validation is mandatory.   

A. Walk-Forward Cross-Validation Blueprint
Walk-Forward Validation (WFV) provides a superior and more realistic simulation of a production trading environment by respecting the temporal order of the data.   

Temporal Integrity Mandate: The primary directive for WFV is to strictly maintain temporal ordering, ensuring that the validation or test set always occurs after the training set. This prevents the model from being trained on information that would not have been available in real-time.   

WFV Implementation: WFV involves a rolling-origin retraining scheme :   

Initial Training Window: A fixed minimum historical window (e.g., 5 years of daily data) is defined for the initial model training.

Forecast Horizon: The model predicts the outcomes for a specific period immediately following the training window (the forecast horizon, e.g., the next 22 trading days).

Walk Forward Step: After evaluation, the training window's origin is "rolled forward" by the length of the forecast horizon, and the model is retrained on the updated, larger dataset before predicting the next horizon.   

Advantages and Adaptation: WFV provides a reliable estimate of real-world performance because it simulates the need for periodic retraining and updating. This technique is vital for detecting and adapting to changes in underlying data patterns over time, such as shifts in market regimes or volatility clustering, which occur frequently in financial markets.   

Hyperparameter Optimization: Hyperparameter search (e.g., tuning the optimal LSTM sequence length L, the learning rates, and XGBoost parameters) must be executed using nested WFV. Tools like Optuna are essential for automatic, efficient hyperparameter tuning within the resource constraints. Libraries such as Darts or backtesting.py provide built-in support for time-aware validation strategies that respect temporal ordering.   

B. Metrics for Imbalanced Financial Classification
Financial classification problems, particularly directional movement prediction, are inherently subject to high class imbalance, as large price moves (the target class) are relatively rare. Relying solely on standard accuracy leads to models that trivially predict the majority class (no significant move or small move), making them useless for practical investment decisions. Specialized metrics are required to quantify risk and reward.   

Balanced Accuracy: This metric is crucial for imbalanced datasets. It is calculated as the average of Sensitivity (True Positive Rate) and Specificity (True Negative Rate). By averaging the performance across both the positive (Up/Down) and negative (No Change) classes, Balanced Accuracy provides a reliable, non-biased measure of the model’s discriminative power, preventing the classifier from favoring the more frequent outcome.   

F-beta Score (Risk-Adjusted Metric): The F-beta score generalizes the F1 score by allowing the developer to weight the importance of Precision versus Recall. This weighting is directly tied to the risk profile and operational strategy of the deployed system. The generalized formula is:   

F 
β
​
 = 
(β 
2
 ⋅Precision+Recall)
(1+β 
2
 )⋅Precision⋅Recall
​
 
Prioritizing Precision (β=0.5): If the operational goal is to minimize False Positives (incorrect buy/sell signals) to control transaction costs and capital risk, a F 
0.5
​
  score is prioritized. This yields a highly conservative, high-conviction trading strategy.   

Prioritizing Recall (β=2): If the objective is maximizing the capture of all true market opportunities, even at the expense of a higher rate of false signals, an F 
2
​
  score is preferred. This indicates a focus on minimizing False Negatives (missed trading opportunities).   

Directional Accuracy (DA) and AUC-ROC: Directional Accuracy, measuring the percentage of correctly predicted upward or downward movements, provides a necessary but insufficient baseline. The Area Under the Receiver Operating Characteristic Curve (AUC-ROC) remains an essential measure of the model's overall discriminative capability across all classification thresholds.   

The following table summarizes the strategic implications of choosing the appropriate evaluation metric for a financial classifier:

Table: Key Performance Metrics and Strategic Implication

Metric	Formula Basis	Significance for Quant Developer	Strategic Trading Implication
F-beta Score (β=0.5)	
Weighted harmonic mean (Precision-focused) 

Maximizes confidence in positive predictions (Precision).	Low Turnover Strategy: Ensures signals are high-conviction, minimizing transaction fees from bad trades.
F-beta Score (β=2)	
Weighted harmonic mean (Recall-focused) 

Maximizes capture of true market movements (Recall).	High Turnover Strategy: Prioritizes spotting market opportunities, accepting higher rate of false signals.
Balanced Accuracy	
Average of Sensitivity (TPR) and Specificity (TNR) 

Provides reliable evaluation in high-imbalance scenarios.	Essential baseline measure of model fairness across classes (Up/Down).
  
VI. The Open-Source Implementation Roadmap and Scalability
Successful deployment of the hybrid classifier requires a meticulous and organized open-source implementation roadmap, focusing particularly on managing the data heterogeneity and temporal complexity.

A. Final Data Pipeline Summary
The data pipeline must be automated and sequentially executed to prevent leakage and ensure consistency:

Data Ingestion and Alignment: Pull Technical data via Alpha Vantage/Finnhub and Macro data via DBnomics/Data.gov. Load the FNSPID corpus into Pandas DataFrames. Critical time alignment of low-frequency Macro data must be performed.

Feature Computation: Apply TA-Lib for technical features. Utilize the pre-trained FinBERT model via Hugging Face to generate Sentiment scores and dense embeddings. Apply PCA (using scikit-learn) to reduce the dimensionality of both Macro indicators and Sentiment embeddings.   

Scaling and Transformation: Apply sklearn scalers (MinMaxScaler) only to the features destined for the LSTM component. Subsequently, the data must be rigorously transformed into the 3D tensor shape [N 
samples
​
 ,L,N 
features
​
 ] necessary for LSTM input.   

Train/Test Split: Implement the Walk-Forward Splitter (using custom code or libraries like Darts) to partition the data, respecting temporal order for training and validation.   

B. Model Training and Optimization
The hybrid nature of the model necessitates a complex training loop synchronized with the WFV strategy.

Base Model Training (WFV): The PyTorch LSTM model and the XGBoost Base Classifier are trained iteratively on the rolling training window defined by the WFV protocol. Crucially, the prediction outputs from each base model must be collected as out-of-fold data points, which will form the training set for the meta-classifier.   

Meta-Model Training: The final XGBoost Meta-Classifier is trained only on the concatenated out-of-fold predictions and raw features. This ensures that the meta-learner generalizes correctly to unseen data, having learned from the base models' errors on their respective validation folds.   

Hyperparameter Optimization: Tools like Optuna or scikit-optimize should be integrated to perform simultaneous optimization of the hyperparameter sets for both the LSTM (e.g., sequence length L, number of units, learning rate) and XGBoost (e.g., n_estimators, max_depth) within the nested WFV framework.   

Pipeline Management: Due to the complexity—involving data transformation (2D to 3D), scaling, feature selection, and the sequential nature of WFV—a unified pipeline utility (e.g., leveraging sklearn.pipeline constructs) is essential to maintain feature alignment, consistency, and integrity throughout the process, ensuring no data leakage occurs across the folds.   

C. Addressing Non-Stationarity and Future Research
The effectiveness of financial models declines as market characteristics change (concept drift). The WFV process mitigates this by continuous retraining, forcing the model to prioritize recent data. However, ongoing monitoring is required. Feature importance analysis via XGBoost’s SHAP values offers a key diagnostic tool, revealing if the underlying drivers of the classifier are shifting—for example, if the model begins relying more heavily on sentiment features during periods of high news volatility compared to stable periods where technical indicators dominate.   

Future research could explore several architectural enhancements without violating the open-source mandate:

Alternative GBDT Algorithms: LightGBM or CatBoost could be benchmarked against XGBoost. LightGBM, utilizing leaf-wise growth, is typically significantly faster and more efficient, though XGBoost is often cited as building more robust models.   

Deep Learning Enhancements: Incorporating Attention Mechanisms (AM) into the LSTM architecture (CNN-LSTM-AM hybrids) could enhance the model’s ability to selectively focus on the most relevant parts of the input sequence, potentially yielding superior performance metrics in financial time series prediction.   

VII. Conclusions and Recommendations
The Macro-Technical Sentiment Classifier, realized through a fully open-source XGBoost-LSTM stacking architecture, represents a methodologically robust approach to directional financial classification. The design explicitly addresses the inherent challenges of financial time series by segmenting the predictive effort: the LSTM effectively models temporal patterns and complex sentiment sequences, while XGBoost handles non-linear interactions within the tabular Macro and Technical features, and ultimately combines the ensemble outputs.

The structural decision to employ a Stacking Ensemble with XGBoost as the meta-classifier is critical. This configuration allows the system to synthesize specialized predictions from distinct data domains—the long-term memory of the LSTM and the instantaneous non-linear feature mapping of the GBDT—yielding a classification system that is superior to its standalone counterparts.   

For practical implementation, two strategic recommendations derived from the analysis are necessary:

Mandatory Walk-Forward Validation: Due to the non-stationary nature of financial data, implementation must strictly adhere to the Walk-Forward Validation protocol. This ensures that performance metrics are realistic, and that the model is continuously adapted to evolving market regimes, minimizing the risk of deployment failure due to data leakage.   

Risk-Adjusted Metric Selection: Given the operational context of financial trading, the performance metric must reflect strategic risk tolerance. The F-beta score is recommended for optimization, with the specific β value (F 
0.5
​
  for minimizing False Positives or F 
2
​
  for maximizing Recall) determined by the required risk profile of the resulting trading strategy. Balanced Accuracy should be used concurrently to monitor fairness across the minority and majority classes.   

